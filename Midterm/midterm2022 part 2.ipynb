{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTOX0d3cmLQf"
   },
   "source": [
    "# Midterm Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFQSh4jlh9Ac"
   },
   "source": [
    "In this midterm, we will compare optimization algorithms for linear regression.\n",
    "- linear regression X (analytic solution, gradient descent, adam)\n",
    "without using torch.nn / torch.optim packages.\n",
    "\n",
    "For this exam, you need to \n",
    "1. fill out ``midterm.py``\n",
    "2. run the current jupyter notebook\n",
    "3. explain the two algorithms GD & ADAM by comparing your results (answer at the end of this notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cnf0BfHZfWzO"
   },
   "source": [
    "## Google Colab Setup\n",
    "Setup the work environment following Homework 1 (mount google drive, set the path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrfeHl_-m4V-"
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyQblYp0nEZq"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvinzSr2fCW7"
   },
   "source": [
    "## Generate a simulation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import midterm\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "midterm.reset_seed(1)\n",
    "trainX, trainy, true_beta = midterm.gen_linear(N=500,D=10)\n",
    "testX, testy, beta2 = midterm.gen_linear(N=500,D=10, beta=torch.squeeze(true_beta))\n",
    "print(true_beta-beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit linear regression with analytic solution\n",
    "Here, you need to fill out ``midterm.py`` file (``train_analytic``, ``predict`` functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = midterm.LinearRegress(trainX,trainy)\n",
    "model_a.train_analytic()\n",
    "print('Training Mean squared error (MSE):', (model_a.beta-true_beta).square().mean())\n",
    "print('Training Mean squared prediction error (MSPE):', (model_a.yhat_train-trainy).square().mean())\n",
    "testy_hat = model_a.predict(testX)\n",
    "print('Test Mean squared prediction error (MSPE):', (testy_hat-testy).square().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge with analytic solution\n",
    "There is no additional codding required. We can just modify $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.train_analytic(lam=.5)\n",
    "print('Training Mean squared error (MSE):', (model_a.beta-true_beta).square().mean())\n",
    "print('Training Mean squared prediction error (MSPE):', (model_a.yhat_train-trainy).square().mean())\n",
    "testy_hat = model_a.predict(testX)\n",
    "print('Test Mean squared prediction error (MSPE):', (testy_hat-testy).square().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gradient descent\n",
    "Let's use the MSPE as our loss function,\n",
    "$$ L(y,beta) =\\frac{1}{N} \\sum (y_i-({\\beta}_0 + {\\beta}_1 x_{i1}+ \\ldots + {\\beta}_D x_{iD})^2, $$\n",
    "because we do not know the true ``beta`` in a real case.\n",
    "\n",
    "With a matrix form (and a proper modification), \n",
    "$$ L(y,beta) = \\frac{1}{N}\\|y-X\\beta\\|^2 $$\n",
    "Write down your gradient descent method code in ``train_gd``. Don't forget to consider $\\frac{1}{n}$. Without it, your gradient might be too large (change the learning rate). \n",
    "\n",
    "In the code below, you can modify ``lr`` and ``niter`` as you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gd = midterm.LinearRegress(trainX,trainy)\n",
    "\n",
    "lr = 1e-2\n",
    "niter = 2000\n",
    "model_gd.train_gd(niter = niter, lr=lr)\n",
    "\n",
    "print('Training Mean squared error (MSE):', (model_gd.beta-true_beta).square().mean())\n",
    "print('Training Mean squared prediction error (MSPE):', (model_gd.yhat_train-trainy).square().mean())\n",
    "testy_hat = model_gd.predict(testX)\n",
    "print('Test Mean squared prediction error (MSPE):', (testy_hat-testy).square().mean())\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(model_gd.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "Let's use adam as your optimizer. Fill out the ``train_adam``. In your code, you can add ``t=t+1`` at the beginning to make the iteration number start from 1 instead of 0. \n",
    "- Try to modify your ``lr`` to see how sensitive it is. \n",
    "- Try to modify ``niter`` to see how fast it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = midterm.LinearRegress(trainX,trainy)\n",
    "\n",
    "lr = 1\n",
    "niter = 200\n",
    "model_adam.train_adam(niter = niter, lr=lr, beta1=0.9, beta2=0.999)\n",
    "\n",
    "print('Training Mean squared error (MSE):', (model_adam.beta-true_beta).square().mean())\n",
    "print('Training Mean squared prediction error (MSPE):', (model_adam.yhat_train-trainy).square().mean())\n",
    "testy_hat = model_adam.predict(testX)\n",
    "print('Test Mean squared prediction error (MSPE):', (testy_hat-testy).square().mean())\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(model_adam.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare test MSPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy_hat = model_a.predict(testX)\n",
    "print('Analytic-Test Mean squared prediction error (MSPE):', (testy_hat-testy).square().mean())\n",
    "\n",
    "testy_hat = model_gd.predict(testX)\n",
    "print('GD-Test Mean squared prediction error (MSPE):', (testy_hat-testy).square().mean())\n",
    "\n",
    "testy_hat = model_adam.predict(testX)\n",
    "print('adam-Test Mean squared prediction error (MSPE):', (testy_hat-testy).square().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the two optimization algorithms with comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill out your answer below. A paragraph would be fine!\n",
    "answer: "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMznzdjG8feKItSv1roKtdg",
   "collapsed_sections": [],
   "name": "midterm2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
