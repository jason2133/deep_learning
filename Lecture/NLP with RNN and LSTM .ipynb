{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48e6ee20",
      "metadata": {
        "id": "48e6ee20"
      },
      "source": [
        "# RNN Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "PF-fGgCG2K1W"
      },
      "id": "PF-fGgCG2K1W"
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "9Xs7X_Fz1-Fm"
      },
      "id": "9Xs7X_Fz1-Fm",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "#print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "import sys\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfAZ9fIy1_tU",
        "outputId": "70d7327b-d8cd-4d10-cee7-200eb1e05429"
      },
      "id": "yfAZ9fIy1_tU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5de6a20",
      "metadata": {
        "id": "a5de6a20"
      },
      "source": [
        "# Implementation of RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3161563f",
      "metadata": {
        "id": "3161563f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea3c232",
      "metadata": {
        "id": "dea3c232"
      },
      "source": [
        "## Creating Vocabulary Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9fec2392",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fec2392",
        "outputId": "106f3575-907b-441a-a47b-f0741286078a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique chars {'a', 'u', 'h', 'o', 'y', 'n', 'm', 'd', 'v', 'c', 'i', 'f', 'e', 'w', ' ', 'r', 'g'}\n",
            "integer mappnig {0: 'a', 1: 'u', 2: 'h', 3: 'o', 4: 'y', 5: 'n', 6: 'm', 7: 'd', 8: 'v', 9: 'c', 10: 'i', 11: 'f', 12: 'e', 13: 'w', 14: ' ', 15: 'r', 16: 'g'}\n",
            "dictionary {'a': 0, 'u': 1, 'h': 2, 'o': 3, 'y': 4, 'n': 5, 'm': 6, 'd': 7, 'v': 8, 'c': 9, 'i': 10, 'f': 11, 'e': 12, 'w': 13, ' ': 14, 'r': 15, 'g': 16}\n"
          ]
        }
      ],
      "source": [
        "text = ['hey how are you','good i am fine','have a nice day']\n",
        "\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(''.join(text))\n",
        "print('unique chars',chars)\n",
        "\n",
        "# Creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "print('integer mappnig', int2char)\n",
        "\n",
        "# Creating another dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}\n",
        "print('dictionary',char2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2851e017",
      "metadata": {
        "id": "2851e017"
      },
      "source": [
        "We can see the space(' ') is also included as an element of the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb6950d",
      "metadata": {
        "id": "fbb6950d"
      },
      "source": [
        "# Padding and splitting into input/labels\n",
        "Next, we'll be padding our input sentences to ensure that all the sentences are of the sample length. While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in batches to speed up the training process. In order to used batches to train on our data, we'll need to ensure that each sequence within the input data are of equal size.\n",
        "\n",
        "Therefore, in most cases, padding can be done by filling up sequences that are too short with 0 values and trimming sequences that are too long. In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e5393f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e5393f0",
        "outputId": "42435b96-7115-4ae0-80e6-1371ae2bb5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey how are you\n",
            "The longest string has 15 characters\n"
          ]
        }
      ],
      "source": [
        "print(max(text,key=len))\n",
        "maxlen = len(max(text, key=len))\n",
        "print(\"The longest string has {} characters\".format(maxlen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9b0ee1da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b0ee1da",
        "outputId": "33c624e0-ef51-4654-bd48-0c6a68bdc9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hey how are you', 'good i am fine ', 'have a nice day']\n"
          ]
        }
      ],
      "source": [
        "# Padding\n",
        "\n",
        "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
        "# the length of the longest sentence\n",
        "for i in range(len(text)):\n",
        "    while len(text[i])<maxlen:\n",
        "        text[i] += ' '\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02be56c",
      "metadata": {
        "id": "a02be56c"
      },
      "source": [
        "As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into\n",
        "\n",
        "- Input data\n",
        "    - The last input character should be excluded as it does not need to be fed into the model\n",
        "- Target/Ground Truth Label\n",
        "    - One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "36e1b3aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36e1b3aa",
        "outputId": "29be5d17-d9d8-4505-e376-1f3084385d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: hey how are yo\n",
            "Target Sequence: ey how are you\n",
            "Input Sequence: good i am fine\n",
            "Target Sequence: ood i am fine \n",
            "Input Sequence: have a nice da\n",
            "Target Sequence: ave a nice day\n"
          ]
        }
      ],
      "source": [
        "# Creating lists that will hold our input and target sequences\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    # Remove last character for input sequence\n",
        "    input_seq.append(text[i][:-1])\n",
        "    \n",
        "    # Remove firsts character for target sequence\n",
        "    target_seq.append(text[i][1:])\n",
        "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef27501",
      "metadata": {
        "id": "7ef27501"
      },
      "source": [
        "Now we can convert our input and target sequences to sequences of integers instead of characters by mapping them using the dictionaries we created above. This will allow us to one-hot-encode our input sequence subsequently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c0e3af43",
      "metadata": {
        "id": "c0e3af43"
      },
      "outputs": [],
      "source": [
        "for i in range(len(text)):\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bfd7908d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfd7908d",
        "outputId": "730f184f-d9a4-42ec-9a01-45c70225cabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 12, 4, 14, 2, 3, 13, 14, 0, 15, 12, 14, 4, 3], [16, 3, 3, 7, 14, 10, 14, 0, 6, 14, 11, 10, 5, 12], [2, 0, 8, 12, 14, 0, 14, 5, 10, 9, 12, 14, 7, 0]]\n"
          ]
        }
      ],
      "source": [
        "print(input_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3567d8",
      "metadata": {
        "id": "2a3567d8"
      },
      "source": [
        "Before encoding our input sequence into one-hot vectors, we'll define 3 key variables:\n",
        "\n",
        "- ``dict_size``: The number of unique characters that we have in our text\n",
        "    - This will determine the one-hot vector size as each character will have an assigned index in that vector\n",
        "- ``seq_len``: The length of the sequences that we're feeding into the model\n",
        "    - As we standardised the length of all our sentences to be equal to the longest sentences, this value will be the ``max length - 1`` as we removed the last character input as well\n",
        "- ``batch_size``: The number of sentences that we defined and are going to feed into the model as a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bc52f345",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc52f345",
        "outputId": "985e6827-e9cd-4ecb-af28-2a30a25fc28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'u': 1, 'h': 2, 'o': 3, 'y': 4, 'n': 5, 'm': 6, 'd': 7, 'v': 8, 'c': 9, 'i': 10, 'f': 11, 'e': 12, 'w': 13, ' ': 14, 'r': 15, 'g': 16}\n"
          ]
        }
      ],
      "source": [
        "print(char2int)\n",
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(text)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, sequence[i][u]] = 1\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ae3af411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae3af411",
        "outputId": "a6faea37-0789-44c0-8cde-c7fe6a103ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
          ]
        }
      ],
      "source": [
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
        "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98c6fa4",
      "metadata": {
        "id": "d98c6fa4"
      },
      "source": [
        "## Move to PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5f4f03a3",
      "metadata": {
        "id": "5f4f03a3"
      },
      "outputs": [],
      "source": [
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "03dfd50c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03dfd50c",
        "outputId": "ff7d4678-52f6-44aa-f774-99456196161d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target tensor([[12.,  4., 14.,  2.,  3., 13., 14.,  0., 15., 12., 14.,  4.,  3.,  1.],\n",
            "        [ 3.,  3.,  7., 14., 10., 14.,  0.,  6., 14., 11., 10.,  5., 12., 14.],\n",
            "        [ 0.,  8., 12., 14.,  0., 14.,  5., 10.,  9., 12., 14.,  7.,  0.,  4.]])\n"
          ]
        }
      ],
      "source": [
        "print('target',target_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff684a67",
      "metadata": {
        "id": "ff684a67"
      },
      "source": [
        "You may run this on GPU if you have much larger dataset size. But in this notebook, we will just use CPU since it's a very small task and doesn't need GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7ce95db6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ce95db6",
        "outputId": "172258f3-fbaa-4279-b9ef-5ab298fbcf0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d50e4d3",
      "metadata": {
        "id": "8d50e4d3"
      },
      "source": [
        "# Defining the model\n",
        "\n",
        "We will use a single-layer RNN followed by a fully connected layer. \n",
        "\n",
        "## ``nn.RNN``\n",
        "\n",
        "Let's see the structure of ``nn.RNN`` first. For full details, see https://pytorch.org/docs/stable/generated/torch.nn.RNN.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5cdcc78c",
      "metadata": {
        "id": "5cdcc78c"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d49d21a",
      "metadata": {
        "id": "2d49d21a"
      },
      "source": [
        "We will train the model witih ``adam`` optimizer and the cross entropy loss since it's a classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d1bae5da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1bae5da",
        "outputId": "2c48a9e3-30c1-4690-9fd7-d3ad59ad5b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (rnn): RNN(17, 12, batch_first=True)\n",
            "  (fc): Linear(in_features=12, out_features=17, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "print(model)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 1000\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "972875b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "972875b3",
        "outputId": "dead7c6f-0179-40c6-fa99-1440b65727a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input 1\n",
            " tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "output 1\n",
            " tensor([12.,  4., 14.,  2.,  3., 13., 14.,  0., 15., 12., 14.,  4.,  3.,  1.])\n"
          ]
        }
      ],
      "source": [
        "print('input 1\\n',input_seq[0])\n",
        "print('output 1\\n',target_seq[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a3e890",
      "metadata": {
        "id": "66a3e890"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "570f863c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "570f863c",
        "outputId": "7fabcf37-4af5-4f3d-e601-77adf2120df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/1000............. Loss: 2.4414\n",
            "Epoch: 20/1000............. Loss: 2.1487\n",
            "Epoch: 30/1000............. Loss: 1.7714\n",
            "Epoch: 40/1000............. Loss: 1.3261\n",
            "Epoch: 50/1000............. Loss: 0.9323\n",
            "Epoch: 60/1000............. Loss: 0.6424\n",
            "Epoch: 70/1000............. Loss: 0.4501\n",
            "Epoch: 80/1000............. Loss: 0.3192\n",
            "Epoch: 90/1000............. Loss: 0.2321\n",
            "Epoch: 100/1000............. Loss: 0.1762\n",
            "Epoch: 110/1000............. Loss: 0.1406\n",
            "Epoch: 120/1000............. Loss: 0.1171\n",
            "Epoch: 130/1000............. Loss: 0.1010\n",
            "Epoch: 140/1000............. Loss: 0.0896\n",
            "Epoch: 150/1000............. Loss: 0.0811\n",
            "Epoch: 160/1000............. Loss: 0.0747\n",
            "Epoch: 170/1000............. Loss: 0.0696\n",
            "Epoch: 180/1000............. Loss: 0.0655\n",
            "Epoch: 190/1000............. Loss: 0.0621\n",
            "Epoch: 200/1000............. Loss: 0.0593\n",
            "Epoch: 210/1000............. Loss: 0.0569\n",
            "Epoch: 220/1000............. Loss: 0.0548\n",
            "Epoch: 230/1000............. Loss: 0.0530\n",
            "Epoch: 240/1000............. Loss: 0.0515\n",
            "Epoch: 250/1000............. Loss: 0.0501\n",
            "Epoch: 260/1000............. Loss: 0.0489\n",
            "Epoch: 270/1000............. Loss: 0.0478\n",
            "Epoch: 280/1000............. Loss: 0.0468\n",
            "Epoch: 290/1000............. Loss: 0.0460\n",
            "Epoch: 300/1000............. Loss: 0.0452\n",
            "Epoch: 310/1000............. Loss: 0.0444\n",
            "Epoch: 320/1000............. Loss: 0.0438\n",
            "Epoch: 330/1000............. Loss: 0.0432\n",
            "Epoch: 340/1000............. Loss: 0.0426\n",
            "Epoch: 350/1000............. Loss: 0.0421\n",
            "Epoch: 360/1000............. Loss: 0.0417\n",
            "Epoch: 370/1000............. Loss: 0.0412\n",
            "Epoch: 380/1000............. Loss: 0.0408\n",
            "Epoch: 390/1000............. Loss: 0.0405\n",
            "Epoch: 400/1000............. Loss: 0.0401\n",
            "Epoch: 410/1000............. Loss: 0.0398\n",
            "Epoch: 420/1000............. Loss: 0.0395\n",
            "Epoch: 430/1000............. Loss: 0.0392\n",
            "Epoch: 440/1000............. Loss: 0.0390\n",
            "Epoch: 450/1000............. Loss: 0.0387\n",
            "Epoch: 460/1000............. Loss: 0.0385\n",
            "Epoch: 470/1000............. Loss: 0.0383\n",
            "Epoch: 480/1000............. Loss: 0.0381\n",
            "Epoch: 490/1000............. Loss: 0.0379\n",
            "Epoch: 500/1000............. Loss: 0.0377\n",
            "Epoch: 510/1000............. Loss: 0.0376\n",
            "Epoch: 520/1000............. Loss: 0.0374\n",
            "Epoch: 530/1000............. Loss: 0.0373\n",
            "Epoch: 540/1000............. Loss: 0.0371\n",
            "Epoch: 550/1000............. Loss: 0.0370\n",
            "Epoch: 560/1000............. Loss: 0.0369\n",
            "Epoch: 570/1000............. Loss: 0.0367\n",
            "Epoch: 580/1000............. Loss: 0.0366\n",
            "Epoch: 590/1000............. Loss: 0.0365\n",
            "Epoch: 600/1000............. Loss: 0.0364\n",
            "Epoch: 610/1000............. Loss: 0.0363\n",
            "Epoch: 620/1000............. Loss: 0.0362\n",
            "Epoch: 630/1000............. Loss: 0.0361\n",
            "Epoch: 640/1000............. Loss: 0.0361\n",
            "Epoch: 650/1000............. Loss: 0.0360\n",
            "Epoch: 660/1000............. Loss: 0.0359\n",
            "Epoch: 670/1000............. Loss: 0.0358\n",
            "Epoch: 680/1000............. Loss: 0.0357\n",
            "Epoch: 690/1000............. Loss: 0.0357\n",
            "Epoch: 700/1000............. Loss: 0.0356\n",
            "Epoch: 710/1000............. Loss: 0.0355\n",
            "Epoch: 720/1000............. Loss: 0.0355\n",
            "Epoch: 730/1000............. Loss: 0.0354\n",
            "Epoch: 740/1000............. Loss: 0.0354\n",
            "Epoch: 750/1000............. Loss: 0.0353\n",
            "Epoch: 760/1000............. Loss: 0.0353\n",
            "Epoch: 770/1000............. Loss: 0.0352\n",
            "Epoch: 780/1000............. Loss: 0.0352\n",
            "Epoch: 790/1000............. Loss: 0.0351\n",
            "Epoch: 800/1000............. Loss: 0.0351\n",
            "Epoch: 810/1000............. Loss: 0.0350\n",
            "Epoch: 820/1000............. Loss: 0.0350\n",
            "Epoch: 830/1000............. Loss: 0.0349\n",
            "Epoch: 840/1000............. Loss: 0.0349\n",
            "Epoch: 850/1000............. Loss: 0.0349\n",
            "Epoch: 860/1000............. Loss: 0.0348\n",
            "Epoch: 870/1000............. Loss: 0.0348\n",
            "Epoch: 880/1000............. Loss: 0.0348\n",
            "Epoch: 890/1000............. Loss: 0.0347\n",
            "Epoch: 900/1000............. Loss: 0.0347\n",
            "Epoch: 910/1000............. Loss: 0.0347\n",
            "Epoch: 920/1000............. Loss: 0.0346\n",
            "Epoch: 930/1000............. Loss: 0.0346\n",
            "Epoch: 940/1000............. Loss: 0.0346\n",
            "Epoch: 950/1000............. Loss: 0.0345\n",
            "Epoch: 960/1000............. Loss: 0.0345\n",
            "Epoch: 970/1000............. Loss: 0.0345\n",
            "Epoch: 980/1000............. Loss: 0.0344\n",
            "Epoch: 990/1000............. Loss: 0.0344\n",
            "Epoch: 1000/1000............. Loss: 0.0344\n"
          ]
        }
      ],
      "source": [
        "# Training Run\n",
        "input_seq = input_seq.to(device)\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    #input_seq = input_seq.to(device)\n",
        "    output, hidden = model(input_seq)\n",
        "    output = output.to(device)\n",
        "    target_seq = target_seq.to(device)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34d2587",
      "metadata": {
        "id": "c34d2587"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "335b8aec",
      "metadata": {
        "id": "335b8aec"
      },
      "outputs": [],
      "source": [
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9052226e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9052226e",
        "outputId": "6f8c6a78-b447-48b7-f237-6093cb6e70eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' ', tensor([[[ 0.8923,  0.9730, -0.9979,  0.7101, -0.9843, -0.9024,  0.9480,\n",
              "           -0.9996,  0.9988, -0.9873,  0.9513,  0.9984]]], device='cuda:0',\n",
              "        grad_fn=<CudnnRnnBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "predict(model, 'good')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b2be68",
      "metadata": {
        "id": "56b2be68"
      },
      "source": [
        "Next character is ' '. Let's generate the sentence in a sequential way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "175e7829",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "175e7829",
        "outputId": "b55c4617-6e50-4119-adc2-d5e3f64828ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('o', tensor([[[ 0.3956,  0.7752, -0.9768,  0.7956, -0.9193, -0.2155,  0.7788,\n",
              "            0.7800, -0.7440, -0.9594, -0.3289, -0.5743]]], device='cuda:0',\n",
              "        grad_fn=<CudnnRnnBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "predict(model, 'g')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6eb138c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eb138c9",
        "outputId": "7c02f499-c553-4ddf-e4e5-b968e88946a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('i', tensor([[[ 0.9998,  0.9995, -0.8725,  0.9944,  0.8340,  0.9014,  0.9879,\n",
              "           -0.9575,  0.9524,  0.9971,  0.9796,  0.9998]]], device='cuda:0',\n",
              "        grad_fn=<CudnnRnnBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "predict(model, 'good ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d977309b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d977309b",
        "outputId": "d014456e-2b02-4679-9b39-22a64a1f8826"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' ', tensor([[[-0.1528, -0.9916,  0.9044,  0.9478,  0.1106, -0.9992,  0.9846,\n",
              "           -0.5940,  1.0000, -0.8741,  0.9997, -0.7217]]], device='cuda:0',\n",
              "        grad_fn=<CudnnRnnBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "predict(model,'good i')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8b3266d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b3266d8",
        "outputId": "54ca70b5-ab9e-4bb4-c795-7402b7090e52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a', tensor([[[-0.9758,  0.9766, -0.9974, -0.9284, -0.9935,  0.4962, -0.9774,\n",
              "           -0.4553, -0.2701,  0.9987, -0.6804,  0.9999]]], device='cuda:0',\n",
              "        grad_fn=<CudnnRnnBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "predict(model, 'good i ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "51e96049",
      "metadata": {
        "id": "51e96049"
      },
      "outputs": [],
      "source": [
        "def sample(model, out_len, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]  # ['h', 'e', 'y'] for example\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4cc8226e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4cc8226e",
        "outputId": "f67cd142-d23c-4456-e16d-8a5edde705af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good i am fine '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "sample(model, 15, 'good')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4dfb63c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4dfb63c4",
        "outputId": "145a902e-9e51-4598-ceee-39ff42092499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good i am fine youm fine youm '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sample(model, 30, 'g')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5254af25",
      "metadata": {
        "id": "5254af25"
      },
      "source": [
        "# LSTM Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b65f03d",
      "metadata": {
        "id": "6b65f03d"
      },
      "source": [
        "Before we jump into a project with a full dataset, let's just take a look at how the PyTorch LSTM layer really works in practice by visualizing the outputs. We don't need to instantiate a model to see how the layer works. See https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2c11352d",
      "metadata": {
        "id": "2c11352d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 5\n",
        "hidden_dim = 10\n",
        "n_layers = 1\n",
        "\n",
        "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "106e8dcf",
      "metadata": {
        "id": "106e8dcf"
      },
      "source": [
        "Let's create some dummy data to see how the layer takes in the input. As our input dimension is 5, we have to create a tensor of the shape (1, 1, 5) which represents (batch size, sequence length, input dimension).\n",
        "\n",
        "Additionally, we'll have to initialize a hidden state and cell state for the LSTM as this is the first cell. The hidden state and cell state is stored in a tuple with the format (hidden_state, cell_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6a52eb70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a52eb70",
        "outputId": "2ac40012-a902-43b8-cf39-453b5e0a1e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 3, 5])\n",
            "Hidden shape: (torch.Size([1, 1, 10]), torch.Size([1, 1, 10]))\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "seq_len = 3\n",
        "\n",
        "inp = torch.randn(batch_size, seq_len, input_dim)\n",
        "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "hidden = (hidden_state, cell_state)\n",
        "\n",
        "print(\"Input shape: {}\".format(inp.shape))\n",
        "print(\"Hidden shape: ({}, {})\".format(hidden[0].shape, hidden[1].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe17db8",
      "metadata": {
        "id": "bfe17db8"
      },
      "source": [
        "Next, we’ll feed the input and hidden states and see what we’ll get back from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "88606124",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88606124",
        "outputId": "ab3e40f2-0ffd-40b4-ee9b-a86ae4a57642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape:  torch.Size([1, 3, 10])\n",
            "Hidden:  (tensor([[[-0.1780, -0.1436, -0.1630, -0.1567,  0.0924, -0.0373,  0.1726,\n",
            "          -0.0755, -0.1744,  0.1407]]], grad_fn=<StackBackward0>), tensor([[[-0.3908, -0.2441, -0.2622, -0.2477,  0.1494, -0.0828,  0.3305,\n",
            "          -0.1123, -0.3849,  0.4508]]], grad_fn=<StackBackward0>))\n"
          ]
        }
      ],
      "source": [
        "out, hidden = lstm_layer(inp, hidden)\n",
        "print(\"Output shape: \", out.shape)\n",
        "print(\"Hidden: \", hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452a5e3f",
      "metadata": {
        "id": "452a5e3f"
      },
      "source": [
        "we'll need an output at every time step, such as Text Generation, the output of each time step can be extracted directly from the 2nd dimension and fed into a fully connected layer. For text classification tasks, such as Sentiment Analysis, the last output can be taken to be fed into a classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f69ab6c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f69ab6c5",
        "outputId": "3550a21b-e455-40c3-9904-e9285512df54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "# Obtaining the last output in the sequence\n",
        "out = out.squeeze()[-1, :]\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878db59c",
      "metadata": {
        "id": "878db59c"
      },
      "source": [
        "# Sentiment Analysis on Amazon Reviews with LSTM\n",
        "we’ll be using the Amazon customer reviews dataset which can be found on Kaggle. The dataset contains a total of 4 million reviews with each review labeled to be of either positive or negative sentiment. \n",
        "\n",
        "For our data pre-processing steps, we'll be using regex, ``numpy`` and the ``nltk`` (Natural Language Toolkit) library for some simple NLP helper functions. As the data is compressed in the ``bz2`` format, we'll use the Python bz2 module to read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3415d47f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3415d47f",
        "outputId": "2f11879c-dfac-424d-985d-b7022cccf11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import bz2\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9eb3202",
      "metadata": {
        "id": "c9eb3202"
      },
      "source": [
        "## Download the data using the URL below\n",
        "Download the two files and store them on your working directory.\n",
        "- https://storage.googleapis.com/amazonreviews/test.ft.txt.bz2\n",
        "- https://storage.googleapis.com/amazonreviews/train.ft.txt.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e3106658",
      "metadata": {
        "id": "e3106658"
      },
      "outputs": [],
      "source": [
        "train_file = bz2.BZ2File(GOOGLE_DRIVE_PATH+'/train.ft.txt.bz2')\n",
        "test_file = bz2.BZ2File(GOOGLE_DRIVE_PATH+'/test.ft.txt.bz2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "055471cf",
      "metadata": {
        "id": "055471cf"
      },
      "outputs": [],
      "source": [
        "train_file = train_file.readlines()\n",
        "test_file = test_file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "dd6d4abd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6d4abd",
        "outputId": "42638c39-6b02-4897-8e63-3c45bcd3ab6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training reivews: 3600000\n",
            "Number of test reviews: 400000\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training reivews: \" + str(len(train_file)))\n",
        "print(\"Number of test reviews: \" + str(len(test_file)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b747a7dd",
      "metadata": {
        "id": "b747a7dd"
      },
      "source": [
        "## Choose subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d33cb954",
      "metadata": {
        "id": "d33cb954"
      },
      "outputs": [],
      "source": [
        "num_train = 300000 #We're training on the first 500,000 reviews in the dataset\n",
        "num_test = 100000 #Using 100,000 reviews from test set\n",
        "\n",
        "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
        "test_file = [x.decode('utf-8') for x in test_file[:num_test]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "60974b70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60974b70",
        "outputId": "a10c45d5-2b6b-423b-91ba-8e817e6f4862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(train_file[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf12729",
      "metadata": {
        "id": "9bf12729"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da93967e",
      "metadata": {
        "id": "da93967e"
      },
      "source": [
        "Next, we'll have to extract out the labels from the sentences. The data is the format __label__1/2 <sentence>, therefore we can easily split it accordingly. Positive sentiment labels are stored as 1 and negative are stored as 0.\n",
        "\n",
        "We will also change all urls to a standard \"<url>\" as the exact url is irrelevant to the sentiment in most cases.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "893b4b50",
      "metadata": {
        "id": "893b4b50"
      },
      "outputs": [],
      "source": [
        "# Extracting labels from sentences\n",
        "\n",
        "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
        "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
        "\n",
        "    \n",
        "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
        "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n",
        "\n",
        "# Some simple cleaning of data\n",
        "\n",
        "for i in range(len(train_sentences)):\n",
        "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
        "\n",
        "# Modify URLs to <url>\n",
        "\n",
        "for i in range(len(train_sentences)):\n",
        "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
        "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
        "        \n",
        "for i in range(len(test_sentences)):\n",
        "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
        "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab22143",
      "metadata": {
        "id": "1ab22143"
      },
      "source": [
        "### Tokenization\n",
        "After quickly cleaning the data, we will do tokenization of the sentences, which is a standard NLP task. Tokenization is the task of splitting a sentence into the individual tokens, which can be words or punctuation, etc. There are many NLP libraries that are able to do this, such as spaCy or Scikit-learn, but we will be using NLTK here as it has one of the faster tokenizers.\n",
        "\n",
        "The words will then be stored in a dictionary mapping the word to its number of appearances. These words will become our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "450d6244",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450d6244",
        "outputId": "ee29b557-84ee-455d-d084-b50b22937e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0% done\n",
            "6.666666666666667% done\n",
            "13.333333333333334% done\n",
            "20.0% done\n",
            "26.666666666666668% done\n",
            "33.333333333333336% done\n",
            "40.0% done\n",
            "46.666666666666664% done\n",
            "53.333333333333336% done\n",
            "60.0% done\n",
            "66.66666666666667% done\n",
            "73.33333333333333% done\n",
            "80.0% done\n",
            "86.66666666666667% done\n",
            "93.33333333333333% done\n",
            "100% done\n"
          ]
        }
      ],
      "source": [
        "words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "    #The sentences will be stored as a list of words/tokens\n",
        "    train_sentences[i] = []\n",
        "    for word in nltk.word_tokenize(sentence): #Tokenizing the words\n",
        "        words.update([word.lower()]) #Converting all the words to lower case\n",
        "        train_sentences[i].append(word)\n",
        "    if i%20000 == 0:\n",
        "        print(str((i*100)/num_train) + \"% done\")\n",
        "print(\"100% done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3fd323",
      "metadata": {
        "id": "5f3fd323"
      },
      "source": [
        "To remove typos and words that likely don't exist, we'll remove all words from the vocab that only appear once throughout. To account for unknown words and padding, we'll have to add them to our vocabulary as well. Each word in the vocabulary will then be assigned an integer index and thereafter mapped to this integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "43992e85",
      "metadata": {
        "id": "43992e85"
      },
      "outputs": [],
      "source": [
        "# Removing the words that only appear once\n",
        "words = {k:v for k,v in words.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "words = sorted(words, key=words.get, reverse=True)\n",
        "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "words = ['_PAD','_UNK'] + words\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9487e17",
      "metadata": {
        "id": "d9487e17"
      },
      "source": [
        "With the mappings, we'll convert the words in the sentences to their corresponding indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "91330138",
      "metadata": {
        "id": "91330138"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(train_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_sentences[i] = [word2idx[word] if word in word2idx else word2idx['_UNK'] for word in sentence]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    # For test sentences, we have to tokenize the sentences as well\n",
        "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in nltk.word_tokenize(sentence)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25707659",
      "metadata": {
        "id": "25707659"
      },
      "source": [
        "In the last pre-processing step, we'll be padding the sentences with 0s and shortening the lengthy sentences so that the data can be trained in batches to speed things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b2f8b6c2",
      "metadata": {
        "id": "b2f8b6c2"
      },
      "outputs": [],
      "source": [
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
        "\n",
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "48526b2d",
      "metadata": {
        "id": "48526b2d"
      },
      "outputs": [],
      "source": [
        "seq_len = 200 #The length that the sentences will be padded/shortened to\n",
        "\n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "test_sentences = pad_input(test_sentences, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "7aee318a",
      "metadata": {
        "id": "7aee318a"
      },
      "outputs": [],
      "source": [
        "# Converting our labels into numpy arrays\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f9b38e5",
      "metadata": {
        "id": "6f9b38e5"
      },
      "source": [
        "A padded sentence will look something like this, where 0 represents the padding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "90da567a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90da567a",
        "outputId": "40765241-a785-49aa-ebd1-b9e74e908e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0    40   106    13    28  1486  3728    57    31    10     3\n",
            "    40  1811    10    84  1620     2     5    26   925     8    11   106\n",
            "    17   151     6     5   141    89     9     2    69     5   122    14\n",
            "     7    42  1848     9   202    59   241   108     2     7   139  1848\n",
            "    47 35127    38  3195    14     3  2583     2    11   106    47 17011\n",
            "   157     2   967    30     1     1     6   603    47  1280     2    31\n",
            "    10   157    21  2416  4254     2    11    12     7  3461 16717   106\n",
            "    14    28    22     2   182   101   128   146     9   235    12    47\n",
            "   825    59     2  2745     5   272    11     4    72   587   451     4\n",
            "   569     4   402     4   153     4  1667     4  1282  1836   524    31\n",
            "   181    33    80    18    16   879    62    32]\n"
          ]
        }
      ],
      "source": [
        "print(test_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "958ff1a4",
      "metadata": {
        "id": "958ff1a4"
      },
      "outputs": [],
      "source": [
        "# split the training into train/validation\n",
        "split_frac = 0.5\n",
        "split_id = int(split_frac * len(test_sentences))\n",
        "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
        "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d95331f",
      "metadata": {
        "id": "7d95331f"
      },
      "source": [
        "Next, this is the point where we’ll start working with the PyTorch library. We’ll first define the datasets from the sentences and labels, followed by loading them into a data loader. We set the batch size to 256. This can be tweaked according to your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce26c23",
      "metadata": {
        "id": "7ce26c23"
      },
      "source": [
        "### Use Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "d1f18aa9",
      "metadata": {
        "id": "d1f18aa9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
        "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "5afccefe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5afccefe",
        "outputId": "1308e41a-1763-47e9-edc7-ab8a4106d0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "B4tQDi_f3nVf"
      },
      "id": "B4tQDi_f3nVf"
    },
    {
      "cell_type": "markdown",
      "id": "4498dea8",
      "metadata": {
        "id": "4498dea8"
      },
      "source": [
        "At this point, we will be defining the architecture of the model. At this stage, we can create Neural Networks that have deep layers or and large number of LSTM layers stacked on top of each other. However, a simple model such as the one below works quite well and requires much less training time. We will be training our own word embeddings in the first layer before the sentences are fed into the LSTM layer.\n",
        "\n",
        "The final layer is a fully connected layer with a sigmoid function to classify whether the review is of positive/negative sentiment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b9bb3367",
      "metadata": {
        "id": "b9bb3367"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "069b24f5",
      "metadata": {
        "id": "069b24f5"
      },
      "source": [
        "Take note that we can actually load pre-trained word embeddings such as GloVe or fastText which can increase the model’s accuracy and decrease training time.\n",
        "\n",
        "With this, we can instantiate our model after defining the arguments. The output dimension will only be 1 as it only needs to output 1 or 0. The learning rate, loss function and optimizer are defined as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "c45eb9b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c45eb9b0",
        "outputId": "2dc722f2-5b56-4342-8cc0-7ab8a71b887a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentNet(\n",
            "  (embedding): Embedding(121325, 400)\n",
            "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(word2idx) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "4d2defb5",
      "metadata": {
        "id": "4d2defb5"
      },
      "outputs": [],
      "source": [
        "lr=0.005\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17776892",
      "metadata": {
        "id": "17776892"
      },
      "source": [
        "Finally, we can start training the model. For every 1000 steps, we’ll be checking the output of our model against the validation dataset and saving the model if it performed better than the previous time. The state_dict is the model’s weights in PyTorch and can be loaded into a model with the same architecture at a separate time or script altogether."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "19c0d71c",
      "metadata": {
        "id": "19c0d71c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3849564-4b5c-4d1f-edde-34977766fb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/2... Step: 1000... Loss: 0.129837... Val Loss: 0.187997\n",
            "Validation loss decreased (inf --> 0.187997).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "counter = 0\n",
        "print_every = 1000\n",
        "clip = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "    h = model.init_hidden(batch_size)\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        h = tuple([e.data for e in h])\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if counter%print_every == 0:\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inp, lab in val_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                inp, lab = inp.to(device), lab.to(device)\n",
        "                out, val_h = model(inp, val_h)\n",
        "                val_loss = criterion(out.squeeze(), lab.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "            if np.mean(val_losses) <= valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "1ae6afbe",
      "metadata": {
        "id": "1ae6afbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161f8841-e377-4ad5-d938-257f1b672e07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "#Loading the best model\n",
        "model.load_state_dict(torch.load('./state_dict.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "4aa391d6",
      "metadata": {
        "id": "4aa391d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ccd5d5f-db5d-4188-a266-41aa14fdbf42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.186\n",
            "Test accuracy: 93.166%\n"
          ]
        }
      ],
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze()) #rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40643266",
      "metadata": {
        "id": "40643266"
      },
      "source": [
        "This result was achieved with just a few simple layers and without any hyperparameter tuning. There are so many other improvements that can be made to increase the model’s effectiveness, and you are free to attempt to beat this accuracy by implementing these improvements!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test your review"
      ],
      "metadata": {
        "id": "Oj7URk80jMRK"
      },
      "id": "Oj7URk80jMRK"
    },
    {
      "cell_type": "code",
      "source": [
        "test_sen1 = 'it is so amazing.'\n",
        "test_sen_word1 = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in nltk.word_tokenize(test_sen1)]\n",
        "print(test_sen1)\n",
        "print(test_sen_word1)\n",
        "\n",
        "test_sen2 = 'you should be very careful.'\n",
        "test_sen_word2 = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in nltk.word_tokenize(test_sen2)]\n",
        "\n",
        "test_sen3 = 'you should be very careful. I have returned'\n",
        "test_sen_word3 = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in nltk.word_tokenize(test_sen3)]\n",
        "\n",
        "\n",
        "test_sen_word = [test_sen_word1, test_sen_word2, test_sen_word3]\n",
        "test_sen_final =  pad_input(test_sen_word, seq_len)\n",
        "print(test_sen_final)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dqKAJwMgB5_",
        "outputId": "3d05164f-4f0d-4e09-f055-fc8ba4cc2edd"
      },
      "id": "2dqKAJwMgB5_",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it is so amazing.\n",
            "[9, 12, 36, 326, 2]\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    9\n",
            "    12   36  326    2]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0   19  135\n",
            "    34   44 1510    2]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0   19  135   34   44 1510\n",
            "     2    5   26  725]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sen_final_torch = torch.from_numpy(test_sen_final)\n",
        "inp_test = test_sen_final_torch.to(device)\n",
        "h = model.init_hidden(len(test_sen_word))\n",
        "output, h = model(inp_test, h)\n",
        "print('1', test_sen1, '\\n2 ', test_sen2,'\\n3 ',test_sen3)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo06yTJvh1y_",
        "outputId": "4ff505e2-339f-43ee-fdea-350aee3590d0"
      },
      "id": "zo06yTJvh1y_",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 it is so amazing. \n",
            "2  you should be very careful. \n",
            "3  you should be very careful. I have returned\n",
            "tensor([0.9917, 0.4249, 0.1015], device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "RNN, LSTM Basic.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c34d2587"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}