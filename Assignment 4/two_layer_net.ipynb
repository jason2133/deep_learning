{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNmLmqrJAXXp"
   },
   "source": [
    "# Homework 4. Two Layer Neural Network - Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrfeHl_-m4V-"
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650107651217,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "VyQblYp0nEZq"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cnf0BfHZfWzO"
   },
   "source": [
    "## Google Colab Setup\n",
    "Setup the work environment following Homework 1 (load autoload, mount google drive, set the path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5860306,
     "status": "ok",
     "timestamp": 1650113511520,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "Pyzyhj8Sbzwx",
    "outputId": "cf8cd9e6-2a8b-4bbe-cce5-4776f5d6286f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1650113512538,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "y2ucyve0cqrI",
    "outputId": "df7e1faf-1733-4e3e-8121-ed874cf725d8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/HW4'\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "import sys\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6WjZGY8A9CI"
   },
   "source": [
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5758,
     "status": "ok",
     "timestamp": 1650113518294,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "O3EvIZ0uAOVN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvUDZWGU3VLV"
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650113518295,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "RrAX9FOLpr9k",
    "outputId": "26a8bd57-06de-4691-e90d-a6922a4ddf77"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbe3wUpVAjma"
   },
   "source": [
    "# Implementing a Neural Network\n",
    "In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
    "\n",
    "We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. \n",
    "\n",
    "In other words, the network has the following architecture:\n",
    "\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "The outputs of the second fully-connected layer are the scores for each class.\n",
    "\n",
    "**Note**: When you implment the regularization over W, **please DO NOT multiply the regularization term by 1/2** (no coefficient). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJqim3P1qZgv"
   },
   "source": [
    "## Simulation model (toy example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T-4Phbd9GvI"
   },
   "source": [
    "The inputs to our network will be a batch of $N$ (`num_inputs`) $D$-dimensional vectors (`input_size`); the hidden layer will have $H$ hidden units (`hidden_size`), and we will predict classification scores for $C$ categories (`num_classes`). This means that the learnable weights and biases of the network will have the following shapes:\n",
    "\n",
    "*   W1: First layer weights; has shape (D, H)\n",
    "*   b1: First layer biases; has shape (H,)\n",
    "*   W2: Second layer weights; has shape (H, C)\n",
    "*   b2: Second layer biases; has shape (C,)\n",
    "\n",
    "We will use `gen_data.gen_twolayer1` function to generate random weights for a small toy model while we implement the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLdCF3B-AOVT"
   },
   "source": [
    "### Forward pass: compute scores\n",
    "Like in the Linear Classifiers exercise, we want to write a function that takes as input the model weights and a batch of images and labels, and returns the loss and the gradient of the loss with respect to each model parameter.\n",
    "\n",
    "However rather than attempting to implement the entire function at once, we will take a staged approach and ask you to implement the full forward and backward pass one step at a time.\n",
    "\n",
    "First we will implement the forward pass of the network which uses the weights and biases to compute scores for all inputs in `nn_forward_pass`. We will use `subcodes.py` which stores some common functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inlH2l-XEtZQ"
   },
   "source": [
    "Compute the scores and compare with the answer. The distance gap should be smaller than 1e-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1424,
     "status": "ok",
     "timestamp": 1650113519716,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "tZV9_3ZWAOVU",
    "outputId": "1f988d90-a394-45c2-e4f0-8a3d2ea5f0ba"
   },
   "outputs": [],
   "source": [
    "from gen_data import gen_twolayer1\n",
    "from subcodes import reset_seed\n",
    "\n",
    "from two_layer_net import nn_forward_pass\n",
    "\n",
    "reset_seed(0)\n",
    "toy_X, toy_y, params = gen_twolayer1()\n",
    "# Default: N=5,D=4,H=10,C=3 \n",
    "\n",
    "# YOUR_TURN: Implement the score computation part of nn_forward_pass\n",
    "scores, _ = nn_forward_pass(params, toy_X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print(scores.dtype)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = torch.tensor([\n",
    "        [ 4.5398e-07, -1.9270e-07, -4.8966e-07],\n",
    "        [ 5.4361e-07,  9.8901e-08,  7.2228e-07],\n",
    "        [-5.9020e-08,  2.0401e-09,  1.6904e-07],\n",
    "        [ 2.2207e-07,  5.2379e-08,  2.7437e-07],\n",
    "        [ 3.0348e-07, -5.1789e-08, -6.1906e-08]], dtype=torch.float32, device=scores.device)\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-10\n",
    "scores_diff = (scores - correct_scores).abs().sum().item()\n",
    "print('Difference between your scores and correct scores: %.2e' % scores_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XNJ3ydEAOVW"
   },
   "source": [
    "### Forward pass: compute loss\n",
    "Now, we implement the first part of `nn_forward_backward` that computes the data and regularization loss.\n",
    "\n",
    "For the data loss, we will use the softmax loss. For the regularization loss we will use L2 regularization on the weight matrices `W1` and `W2`; we will not apply regularization loss to the bias vectors `b1` and `b2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C734SdJGE6xh"
   },
   "source": [
    "First, Let's run the following to check your implementation.\n",
    "\n",
    "We compute the loss for the toy data, and compare with the answer computed by our implementation. The difference between the correct and computed loss should be less than `1e-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650113519716,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "wgG6w2uKAOVX",
    "outputId": "65e030f0-c208-4f8d-9da7-5d7fc38a60f3"
   },
   "outputs": [],
   "source": [
    "from gen_data import gen_twolayer1\n",
    "from subcodes import reset_seed\n",
    "from two_layer_net import nn_forward_backward\n",
    "\n",
    "reset_seed(0)\n",
    "toy_X, toy_y, params = gen_twolayer1()\n",
    "\n",
    "print('X shape:', toy_X.shape)\n",
    "print('y shape:', toy_y.shape)\n",
    "print('parameters', params)\n",
    "# YOUR_TURN: Implement the loss computation part of nn_forward_backward\n",
    "loss, _ = nn_forward_backward(params, toy_X, toy_y, reg=0.05)\n",
    "print('Your loss: ', loss.item())\n",
    "correct_loss = 1.0986121892929077\n",
    "print('Correct loss: ', correct_loss)\n",
    "diff = (correct_loss - loss).item()\n",
    "\n",
    "# should be very small, we get < 1e-4\n",
    "print('Difference: %.4e' % diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vExP-7n3AOVa"
   },
   "source": [
    "### Backward pass\n",
    "Now implement the backward pass for the entire network in `nn_forward_backward`.\n",
    "\n",
    "After doing so, we will use numeric gradient checking to see whether the analytic gradient computed by our backward pass mateches a numeric gradient.\n",
    "\n",
    "We will use the functions `compute_numeric_gradient` and `rel_error` in `subcodes.py` to help with numeric gradient checking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93oOdibtW_Kl"
   },
   "source": [
    "Now we will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check.\n",
    "\n",
    "You should see relative errors less than `1e-4` for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1650113520129,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "qCEkprvoAOVb",
    "outputId": "e120204d-3467-4fc3-b30a-94a47943d102"
   },
   "outputs": [],
   "source": [
    "from subcodes import compute_numeric_gradient\n",
    "from subcodes import rel_error\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "reg = 0.05\n",
    "toy_X, toy_y, params = gen_twolayer1(dtype=torch.float64)\n",
    "\n",
    "# YOUR_TURN: Implement the gradient computation part of nn_forward_backward\n",
    "#            When you implement the gradient computation part, you may need to \n",
    "#            implement the `hidden` output in nn_forward_pass, as well.\n",
    "loss, grads = nn_forward_backward(params, toy_X, toy_y, reg=reg)\n",
    "\n",
    "for param_name, grad in grads.items():\n",
    "  param = params[param_name]\n",
    "  f = lambda w: nn_forward_backward(params, toy_X, toy_y, reg=reg)[0]\n",
    "  grad_numeric = compute_numeric_gradient(f, param)\n",
    "  error = rel_error(grad, grad_numeric)\n",
    "  print('%s max relative error: %e' % (param_name, error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjAUalCBAOVd"
   },
   "source": [
    "### Train the network\n",
    "To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. \n",
    "\n",
    "Look at the function `nn_train` and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. \n",
    "\n",
    "You will also have to implement `nn_predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains. \n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. Your final training loss should be less than 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1650113520598,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "Wgw06cLXAOVd",
    "outputId": "6c87e5e5-b59e-4443-c271-9364cb0de94e"
   },
   "outputs": [],
   "source": [
    "from two_layer_net import nn_forward_backward, nn_train, nn_predict\n",
    "\n",
    "reset_seed(0)\n",
    "toy_X, toy_y, params = gen_twolayer1()\n",
    "\n",
    "# YOUR_TURN: Implement the nn_train function.\n",
    "#            You may need to check nn_predict function (the \"pred_func\") as well.\n",
    "stats = nn_train(params, nn_forward_backward, nn_predict, toy_X, toy_y, toy_X, toy_y,\n",
    "                 learning_rate=1e-1, reg=1e-6,\n",
    "                 num_iters=200, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'], 'o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1650113521206,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "EUS4aDp_HzG1",
    "outputId": "aef7ef68-8ca9-4b42-a193-461fce1be93e"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.plot(stats['train_acc_history'], 'o', label='train')\n",
    "plt.plot(stats['val_acc_history'], 'o', label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cPIajWNAOVg"
   },
   "source": [
    "## Testing our NN on a real dataset: CIFAR-10\n",
    "Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2893,
     "status": "ok",
     "timestamp": 1650114527734,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "lYo_XrU3AOVg",
    "outputId": "de905409-790e-45cb-94c7-4a5b0d8b5a06"
   },
   "outputs": [],
   "source": [
    "import gen_data\n",
    "# Invoke the above function to get our data.\n",
    "reset_seed(0)\n",
    "data_dict = gen_data.load_cifar10(dtype=torch.float64)\n",
    "print('Train data shape: ', data_dict['X_train'].shape)\n",
    "print('Train labels shape: ', data_dict['y_train'].shape)\n",
    "print('Validation data shape: ', data_dict['X_val'].shape)\n",
    "print('Validation labels shape: ', data_dict['y_val'].shape)\n",
    "print('Test data shape: ', data_dict['X_test'].shape)\n",
    "print('Test labels shape: ', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq-HkgRBAOVQ"
   },
   "source": [
    "### Wrap all function into a Class\n",
    "We will use the class `TwoLayerNet` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are PyTorch tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CsYAv3uAOVi"
   },
   "source": [
    "### Train a network\n",
    "To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2093,
     "status": "ok",
     "timestamp": 1650114532242,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "hgg0QV9DAOVj",
    "outputId": "46452fa3-5630-4b9d-ffdd-74364fccca15"
   },
   "outputs": [],
   "source": [
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "input_size = 3 * 32 * 32\n",
    "hidden_size = 36\n",
    "num_classes = 10\n",
    "\n",
    "# fix random seed before we generate a set of parameters\n",
    "reset_seed(0)\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes, dtype=data_dict['X_train'].dtype, device=data_dict['X_train'].device)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(data_dict['X_train'], data_dict['y_train'],\n",
    "                  data_dict['X_val'], data_dict['y_val'],\n",
    "                  num_iters=500, batch_size=1000,\n",
    "                  learning_rate=1e-2, learning_rate_decay=0.95,\n",
    "                  reg=0.25, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = net.predict(data_dict['X_val'])\n",
    "val_acc = 100.0 * (y_val_pred == data_dict['y_val']).double().mean().item()\n",
    "print('Validation accuracy: %.2f%%' % val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixxgq5RKAOVl"
   },
   "source": [
    "### Debug the training\n",
    "With the default parameters we provided above, you should get a validation accuracy less than 10% on the validation set. This isn't very good.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1650114837028,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "6sYXImDTAOVm",
    "outputId": "18618ad7-8a28-4f23-c3e9-40b573882e13"
   },
   "outputs": [],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "# Your nn_train() returns include the information\n",
    "\n",
    "# loss hitory\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(stats['loss_history'], 'o')\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# train acc / val acc history\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(stats['train_acc_history'], 'o-', label='train')\n",
    "plt.plot(stats['val_acc_history'], 'o-', label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.gcf().set_size_inches(14, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "616EK5UoKgmE"
   },
   "source": [
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, **the first layer weights typically show some visible structure** when visualized.\n",
    "\n",
    "Similar to SVM and Softmax classifier, let's visualize the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "executionInfo": {
     "elapsed": 1211,
     "status": "ok",
     "timestamp": 1650115146791,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "FnuRjtyKAOVo",
    "outputId": "1a1aef25-a603-47d4-9928-db497e000846"
   },
   "outputs": [],
   "source": [
    "from subcodes import visualize_grid\n",
    "# your class object, 'net,' includes the parameter information\n",
    "W1 = net.params['W1']\n",
    "W1 = W1.reshape(3, 32, 32, -1).transpose(0, 3)\n",
    "\n",
    "\n",
    "plt.imshow(visualize_grid(W1, padding=3).type(torch.uint8).cpu())\n",
    "\n",
    "# the above codes will be into subcodes_show_net_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlVbXxmPNzPY"
   },
   "source": [
    "### What's wrong?\n",
    "Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDNZ8ZAnN7hj"
   },
   "source": [
    "#### Capacity?\n",
    "Our initial model has very similar performance on the training and validation sets. This suggests that the model is **underfitting**, and that its performance might improve if we were to increase its capacity.\n",
    "\n",
    "One way we can increase the capacity of a neural network model is to **increase the size of its hidden layer**. Here we investigate the effect of increasing the size of the hidden layer. The performance (as measured by validation-set accuracy) should increase as the size of the hidden layer increases; however it may show diminishing returns for larger layer sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38804,
     "status": "ok",
     "timestamp": 1650115330029,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "_C-ChHUlN68f",
    "outputId": "98bce800-e719-4831-9ca6-62bde2e35658"
   },
   "outputs": [],
   "source": [
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hidden_sizes = [2, 8, 32, 128]  # grid of the sizes of hidden layers\n",
    "lr = 0.1\n",
    "reg = 0.001\n",
    "\n",
    "stat_dict = {}\n",
    "for hs in hidden_sizes:\n",
    "  print('train with hidden size: {}'.format(hs))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  reset_seed(0)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[hs] = stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 1217,
     "status": "ok",
     "timestamp": 1650115344794,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "HpHC1oQuiTw8",
    "outputId": "5db4f518-2f02-4664-fe4d-6766235c0284"
   },
   "outputs": [],
   "source": [
    "# let's plot the accuracies\n",
    "plt.subplot(1, 2, 1)\n",
    "for key, single_stats in stat_dict.items():\n",
    "  plt.plot(single_stats['train_acc_history'], label=str(key))\n",
    "plt.title('Train accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for key, single_stats in stat_dict.items():\n",
    "  plt.plot(single_stats['val_acc_history'], label=str(key))\n",
    "plt.title('Validation accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.gcf().set_size_inches(14, 5)\n",
    "\n",
    "# the above codes will be into subcodes.plot_acc_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpSrK3olUfOZ"
   },
   "source": [
    "#### Regularization?\n",
    "Another possible explanation for the small gap we saw between the train and validation accuracies of our model is **regularization**. In particular, if the regularization coefficient were too high then the model may be unable to fit the training data.\n",
    "\n",
    "We can investigate the phenomenon empirically by training a set of models with varying regularization strengths while fixing other hyperparameters.\n",
    "\n",
    "You should see that setting the regularization strength too high will harm the validation-set performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 52533,
     "status": "error",
     "timestamp": 1650115561722,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "DRPsxxFnU3Un",
    "outputId": "9ddb915e-4887-42bb-8f58-f89069f97248"
   },
   "outputs": [],
   "source": [
    "from subcodes import plot_acc_curves\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hs = 128\n",
    "lr = 1.0\n",
    "regs = [0, 1e-5, 1e-3, 1e-1]\n",
    "\n",
    "stat_dict = {}\n",
    "for reg in regs:\n",
    "  print('train with regularization: {}'.format(reg))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  reset_seed(0)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[reg] = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1650115624569,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "CCiGznTjjeKk",
    "outputId": "090d2f3b-2e73-49ac-bf30-2f384d5aea1e"
   },
   "outputs": [],
   "source": [
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zFWkxebWXtu"
   },
   "source": [
    "#### Learning Rate?\n",
    "Last but not least, we also want to see the effect of learning rate with respect to the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 49990,
     "status": "ok",
     "timestamp": 1650115684116,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "lc_YYCDmWld-",
    "outputId": "21a61a54-c59c-4db8-f985-771d77f41432"
   },
   "outputs": [],
   "source": [
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "hs = 128\n",
    "lrs = [1e-4, 1e-2, 1e0, 1e2]\n",
    "reg = 1e-4\n",
    "\n",
    "stat_dict = {}\n",
    "for lr in lrs:\n",
    "  print('train with learning rate: {}'.format(lr))\n",
    "  # fix random seed before we generate a set of parameters\n",
    "  reset_seed(0)\n",
    "  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n",
    "  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n",
    "            num_iters=3000, batch_size=1000,\n",
    "            learning_rate=lr, learning_rate_decay=0.95,\n",
    "            reg=reg, verbose=False)\n",
    "  stat_dict[lr] = stats\n",
    "\n",
    "plot_acc_curves(stat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVCEro4FAOVq"
   },
   "source": [
    "### Tune your hyperparameters\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, number of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n",
    "\n",
    "**Plots**. To guide your hyperparameter search, you might consider making auxiliary plots of training and validation performance as above, or plotting the results arising from different hyperparameter combinations as we did in the Linear Classifier notebook. You should feel free to plot any auxiliary results you need in order to find a good network, but we don't require any particular plots from you.\n",
    "\n",
    "**Approximate results**. To get full credit for the assignment, you should achieve a classification accuracy above 50% on the validation set.\n",
    "\n",
    "(Our best model gets a validation-set accuracy 56.44% -- did you beat us?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 566573,
     "status": "ok",
     "timestamp": 1650116437111,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "bG4DjBMIAOVq",
    "outputId": "5977ca3f-989f-4c8a-a73e-418a2454362c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from subcodes import plot_stats\n",
    "from two_layer_net import TwoLayerNet, find_best_net, nn_get_search_params\n",
    "\n",
    "# running this model on float64 may needs more time, so set it as float32\n",
    "reset_seed(0)\n",
    "data_dict = gen_data.load_cifar10(dtype=torch.float32)\n",
    "\n",
    "# store the best model into this \n",
    "reset_seed(0)\n",
    "best_net, best_stat, best_val_acc = find_best_net(data_dict, nn_get_search_params)\n",
    "print(best_val_acc)\n",
    "\n",
    "plot_stats(best_stat)\n",
    "\n",
    "# save the best model\n",
    "path = os.path.join(GOOGLE_DRIVE_PATH, 'nn_best_model.pt')\n",
    "best_net.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1650116437881,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "NsYIu49plJ9r",
    "outputId": "bb6cb253-0afd-45e8-866e-4a137ad92b0d"
   },
   "outputs": [],
   "source": [
    "# Check the validation-set accuracy of your best model\n",
    "y_val_preds = best_net.predict(data_dict['X_val'])\n",
    "val_acc = 100 * (y_val_preds == data_dict['y_val']).double().mean().item()\n",
    "print('Best val-set accuracy: %.2f%%' % val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1650116438997,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "hZgDq4zlAOVt",
    "outputId": "5f605dec-84c6-413d-dc68-60267af61e7f"
   },
   "outputs": [],
   "source": [
    "from subcodes import show_net_weights\n",
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG56gKWsAOVv"
   },
   "source": [
    "### Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained network on the test set. To get full credit for the assignment, you should achieve over 50% classification accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650116438998,
     "user": {
      "displayName": "Jun Song",
      "userId": "06262628738862922185"
     },
     "user_tz": -540
    },
    "id": "2b3h8f8_AOVw",
    "outputId": "f52f4c6d-94c1-429b-ca42-0ffa54f4758e"
   },
   "outputs": [],
   "source": [
    "y_test_preds = best_net.predict(data_dict['X_test'])\n",
    "test_acc = 100 * (y_test_preds == data_dict['y_test']).double().mean().item()\n",
    "print('Test accuracy: %.2f%%' % test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "two_layer_net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
